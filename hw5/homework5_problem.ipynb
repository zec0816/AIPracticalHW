{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业五——自然语言处理\n",
    "本次作业的目的是让同学们体验自然语言处理的过程，训练一个将法语翻译成英语的RNN模型。\n",
    "\n",
    "本次作业需要完成的内容：\n",
    "- 仔细阅读代码，补全TODO标记的内容，实现翻译器\n",
    "- 试着调整学习率、`batch_size`、`hidden_size`等超参数，观察它们对实验结果的影响。\n",
    "\n",
    "需要提交的内容：\n",
    "- 补全后的代码 (.py 或 .ipynb 文件)。只需要提交初始版本。\n",
    "- 实验报告（一个PDF文件），要求记录:\n",
    "    - 例句的翻译结果（至少一句，英语和法语都写入报告中）与最终准确率。\n",
    "    - 调整上述超参数得到的实验结果（包括损失函数曲线等），并作简要分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i 1217 08:05:47.847770 64 lock.py:85] Create lock file:/root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/jittor.lock\n",
      "[i 1217 08:05:47.880755 64 compiler.py:956] Jittor(1.3.10.0) src: /root/miniconda3/envs/ai_course/lib/python3.8/site-packages/jittor\n",
      "[i 1217 08:05:47.894729 64 compiler.py:957] g++ at /usr/bin/g++(12.4.0)\n",
      "[i 1217 08:05:47.894888 64 compiler.py:958] cache_path: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default\n",
      "[i 1217 08:05:47.932980 64 install_cuda.py:96] cuda_driver_version: [12, 4]\n",
      "[i 1217 08:05:47.933983 64 install_cuda.py:82] needed restart but not /root/miniconda3/envs/ai_course/bin/python ['-m', 'ipykernel_launcher', '--f=/root/.local/share/jupyter/runtime/kernel-v31cf371cf648a0e0f823aa62fd16e4866d0540853.json'], you can ignore this warning.\n",
      "[i 1217 08:05:47.973033 64 __init__.py:412] Found /root/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc(12.2.140) at /root/.cache/jittor/jtcuda/cuda12.2_cudnn8_linux/bin/nvcc.\n",
      "[i 1217 08:05:47.996321 64 __init__.py:412] Found addr2line(2.42) at /usr/bin/addr2line.\n",
      "[i 1217 08:05:48.227560 64 compiler.py:1013] cuda key:cu12.2.140_sm_89\n",
      "[i 1217 08:05:48.327724 64 compiler.py:34] Create cache dir: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default/cu12.2.140_sm_89\n",
      "[i 1217 08:05:48.328291 64 compiler.py:34] Create cache dir: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default/cu12.2.140_sm_89/jit\n",
      "[i 1217 08:05:48.328580 64 compiler.py:34] Create cache dir: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default/cu12.2.140_sm_89/obj_files\n",
      "[i 1217 08:05:48.328840 64 compiler.py:34] Create cache dir: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default/cu12.2.140_sm_89/gen\n",
      "[i 1217 08:05:48.329093 64 compiler.py:34] Create cache dir: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default/cu12.2.140_sm_89/tmp\n",
      "[i 1217 08:05:48.329326 64 compiler.py:34] Create cache dir: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default/cu12.2.140_sm_89/checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 1217 08:05:57.071027 32 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 1217 08:06:14.918347 32 __init__.py:227] Total mem: 503.57GB, using 16 procs for compiling.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling jittor_core(150/151) used: 28.926s eta: 0.193s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 1217 08:06:55.882061 32 log.cc:351] Load log_sync: 1\u001b[m\n",
      "\u001b[38;5;2m[i 1217 08:06:55.883867 32 jit_compiler.cc:28] Load cc_path: /usr/bin/g++\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling jittor_core(151/151) used: 39.722s eta: 0.000s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 1217 08:06:56.097170 32 init.cc:63] Found cuda archs: [89,]\u001b[m\n",
      "\u001b[38;5;2m[i 1217 08:06:56.140749 32 compile_extern.py:388] Downloading cutt...\u001b[m\n",
      "\u001b[38;5;2m[i 1217 08:06:56.188191 32 compile_extern.py:401] installing cutt...\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling libcutt(8/9) used: 6.115s eta: 0.764s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 1217 08:07:08.775048 32 compiler.py:34] Create cache dir: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default/cu12.2.140_sm_89/custom_ops\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling libcutt(9/9) used: 12.472s eta: 0.000s\n",
      "Compiling gen_ops_mkl_conv_backward_x_mkl_conv_backward_w_mk___hashbc87cd(6/7) used: 3.631s eta: 0.605s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 1217 08:07:15.542422 32 compiler.py:34] Create cache dir: /root/.cache/jittor/jt1.3.10/g++12.4.0/py3.8.20/Linux-5.15.0-1x00/AMDEPYC740224-xf1/176f/default/cu12.2.140_sm_89/cuda\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling gen_ops_mkl_conv_backward_x_mkl_conv_backward_w_mk___hashbc87cd(7/7) used: 4.410s eta: 0.000s\n",
      "Compiling gen_ops_cub_where_cub_test_cub_cumsum_cub_arg_redu___hash7af395(6/6) used: 2.596s eta: 0.000s\n",
      "Compiling gen_ops_cublas_test_cublas_matmul_cublas_acc_matmu___hash10a707(8/8) used: 2.259s eta: 0.000s\n",
      "Compiling gen_ops_cudnn_rnn_cudnn_conv3d_cudnn_rnn_backward____hash4a5ca9(16/16) used: 6.359s eta: 0.000s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[i 1217 08:07:35.972994 32 cuda_flags.cc:55] CUDA enabled.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling gen_ops_cusparse_spmmcoo_cusparse_spmmcsr(5/5) used: 2.198s eta: 0.000s\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import jittor as jt\n",
    "import jittor.optim as optim\n",
    "import jittor.nn as nn\n",
    "from jittor.dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from queue import PriorityQueue\n",
    "jt.flags.use_cuda=True\n",
    "\n",
    "# The max length of sequence\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# Start of Sequence token\n",
    "SOS_token = 0\n",
    "\n",
    "# End of Sequence token\n",
    "EOS_token = 1\n",
    "\n",
    "# Unkown word token\n",
    "UNK_token = 2\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然语言数据的预处理\n",
    "本次实验中，我们仅使用基本的拉丁字母，并将所有句子进行分词处理。\n",
    "\n",
    "我们将语料库中的所有词组成一个词典，每个单词对应一个整数序号。我们通过这种方式将一个句子转变为了一个整数序列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-12-17 08:07:35--  https://cloud.tsinghua.edu.cn/f/d4578477500747d39855/?dl=1\n",
      "Resolving cloud.tsinghua.edu.cn (cloud.tsinghua.edu.cn)... 101.6.15.69, 2402:f000:1:402:101:6:15:69\n",
      "Connecting to cloud.tsinghua.edu.cn (cloud.tsinghua.edu.cn)|101.6.15.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cloud.tsinghua.edu.cn/seafhttp/files/dbb435d4-5c12-442b-927d-a8cf1a5ff309/eng-fra.txt [following]\n",
      "--2025-12-17 08:07:36--  https://cloud.tsinghua.edu.cn/seafhttp/files/dbb435d4-5c12-442b-927d-a8cf1a5ff309/eng-fra.txt\n",
      "Reusing existing connection to cloud.tsinghua.edu.cn:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9541158 (9.1M) [text/plain]\n",
      "Saving to: ‘eng-fra.txt’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  511K 18s\n",
      "    50K .......... .......... .......... .......... ..........  1% 2.05M 11s\n",
      "   100K .......... .......... .......... .......... ..........  1% 2.04M 9s\n",
      "   150K .......... .......... .......... .......... ..........  2% 2.46M 8s\n",
      "   200K .......... .......... .......... .......... ..........  2% 4.00M 6s\n",
      "   250K .......... .......... .......... .......... ..........  3% 2.42M 6s\n",
      "   300K .......... .......... .......... .......... ..........  3% 2.07M 6s\n",
      "   350K .......... .......... .......... .......... ..........  4% 4.02M 5s\n",
      "   400K .......... .......... .......... .......... ..........  4% 4.00M 5s\n",
      "   450K .......... .......... .......... .......... ..........  5% 2.06M 5s\n",
      "   500K .......... .......... .......... .......... ..........  5% 4.02M 4s\n",
      "   550K .......... .......... .......... .......... ..........  6% 2.44M 4s\n",
      "   600K .......... .......... .......... .......... ..........  6% 2.45M 4s\n",
      "   650K .......... .......... .......... .......... ..........  7% 2.46M 4s\n",
      "   700K .......... .......... .......... .......... ..........  8% 3.94M 4s\n",
      "   750K .......... .......... .......... .......... ..........  8% 2.07M 4s\n",
      "   800K .......... .......... .......... .......... ..........  9% 3.99M 4s\n",
      "   850K .......... .......... .......... .......... ..........  9% 2.47M 4s\n",
      "   900K .......... .......... .......... .......... .......... 10% 2.41M 4s\n",
      "   950K .......... .......... .......... .......... .......... 10% 2.06M 4s\n",
      "  1000K .......... .......... .......... .......... .......... 11% 3.97M 4s\n",
      "  1050K .......... .......... .......... .......... .......... 11% 2.05M 4s\n",
      "  1100K .......... .......... .......... .......... .......... 12% 4.02M 4s\n",
      "  1150K .......... .......... .......... .......... .......... 12% 2.05M 4s\n",
      "  1200K .......... .......... .......... .......... .......... 13% 4.01M 3s\n",
      "  1250K .......... .......... .......... .......... .......... 13% 2.05M 3s\n",
      "  1300K .......... .......... .......... .......... .......... 14% 3.98M 3s\n",
      "  1350K .......... .......... .......... .......... .......... 15% 2.42M 3s\n",
      "  1400K .......... .......... .......... .......... .......... 15% 2.07M 3s\n",
      "  1450K .......... .......... .......... .......... .......... 16% 3.95M 3s\n",
      "  1500K .......... .......... .......... .......... .......... 16% 3.07M 3s\n",
      "  1550K .......... .......... .......... .......... .......... 17% 3.06M 3s\n",
      "  1600K .......... .......... .......... .......... .......... 17% 2.04M 3s\n",
      "  1650K .......... .......... .......... .......... .......... 18% 4.05M 3s\n",
      "  1700K .......... .......... .......... .......... .......... 18% 3.06M 3s\n",
      "  1750K .......... .......... .......... .......... .......... 19% 1.73M 3s\n",
      "  1800K .......... .......... .......... .......... .......... 19% 2.47M 3s\n",
      "  1850K .......... .......... .......... .......... .......... 20% 6.03M 3s\n",
      "  1900K .......... .......... .......... .......... .......... 20% 3.08M 3s\n",
      "  1950K .......... .......... .......... .......... .......... 21% 2.43M 3s\n",
      "  2000K .......... .......... .......... .......... .......... 22% 2.05M 3s\n",
      "  2050K .......... .......... .......... .......... .......... 22% 4.00M 3s\n",
      "  2100K .......... .......... .......... .......... .......... 23% 2.46M 3s\n",
      "  2150K .......... .......... .......... .......... .......... 23% 2.41M 3s\n",
      "  2200K .......... .......... .......... .......... .......... 24% 2.06M 3s\n",
      "  2250K .......... .......... .......... .......... .......... 24% 3.96M 3s\n",
      "  2300K .......... .......... .......... .......... .......... 25% 2.47M 3s\n",
      "  2350K .......... .......... .......... .......... .......... 25% 3.08M 3s\n",
      "  2400K .......... .......... .......... .......... .......... 26% 3.01M 3s\n",
      "  2450K .......... .......... .......... .......... .......... 26% 2.44M 3s\n",
      "  2500K .......... .......... .......... .......... .......... 27% 4.00M 3s\n",
      "  2550K .......... .......... .......... .......... .......... 27% 2.48M 3s\n",
      "  2600K .......... .......... .......... .......... .......... 28% 2.02M 3s\n",
      "  2650K .......... .......... .......... .......... .......... 28% 2.44M 3s\n",
      "  2700K .......... .......... .......... .......... .......... 29% 3.03M 3s\n",
      "  2750K .......... .......... .......... .......... .......... 30% 3.06M 3s\n",
      "  2800K .......... .......... .......... .......... .......... 30% 2.07M 3s\n",
      "  2850K .......... .......... .......... .......... .......... 31% 2.96M 3s\n",
      "  2900K .......... .......... .......... .......... .......... 31% 3.09M 2s\n",
      "  2950K .......... .......... .......... .......... .......... 32% 1.53M 2s\n",
      "  3000K .......... .......... .......... .......... .......... 32% 4.04M 2s\n",
      "  3050K .......... .......... .......... .......... .......... 33% 2.43M 2s\n",
      "  3100K .......... .......... .......... .......... .......... 33% 3.00M 2s\n",
      "  3150K .......... .......... .......... .......... .......... 34% 3.02M 2s\n",
      "  3200K .......... .......... .......... .......... .......... 34% 2.08M 2s\n",
      "  3250K .......... .......... .......... .......... .......... 35% 2.44M 2s\n",
      "  3300K .......... .......... .......... .......... .......... 35% 2.45M 2s\n",
      "  3350K .......... .......... .......... .......... .......... 36% 2.43M 2s\n",
      "  3400K .......... .......... .......... .......... .......... 37% 4.06M 2s\n",
      "  3450K .......... .......... .......... .......... .......... 37% 3.01M 2s\n",
      "  3500K .......... .......... .......... .......... .......... 38% 2.44M 2s\n",
      "  3550K .......... .......... .......... .......... .......... 38% 2.46M 2s\n",
      "  3600K .......... .......... .......... .......... .......... 39% 4.02M 2s\n",
      "  3650K .......... .......... .......... .......... .......... 39% 1.76M 2s\n",
      "  3700K .......... .......... .......... .......... .......... 40% 3.06M 2s\n",
      "  3750K .......... .......... .......... .......... .......... 40% 3.06M 2s\n",
      "  3800K .......... .......... .......... .......... .......... 41% 3.06M 2s\n",
      "  3850K .......... .......... .......... .......... .......... 41% 2.43M 2s\n",
      "  3900K .......... .......... .......... .......... .......... 42% 3.97M 2s\n",
      "  3950K .......... .......... .......... .......... .......... 42% 3.10M 2s\n",
      "  4000K .......... .......... .......... .......... .......... 43% 3.03M 2s\n",
      "  4050K .......... .......... .......... .......... .......... 44% 2.04M 2s\n",
      "  4100K .......... .......... .......... .......... .......... 44% 2.42M 2s\n",
      "  4150K .......... .......... .......... .......... .......... 45% 3.12M 2s\n",
      "  4200K .......... .......... .......... .......... .......... 45% 2.05M 2s\n",
      "  4250K .......... .......... .......... .......... .......... 46% 4.02M 2s\n",
      "  4300K .......... .......... .......... .......... .......... 46% 2.44M 2s\n",
      "  4350K .......... .......... .......... .......... .......... 47% 2.47M 2s\n",
      "  4400K .......... .......... .......... .......... .......... 47% 5.96M 2s\n",
      "  4450K .......... .......... .......... .......... .......... 48% 2.01M 2s\n",
      "  4500K .......... .......... .......... .......... .......... 48% 2.49M 2s\n",
      "  4550K .......... .......... .......... .......... .......... 49% 2.43M 2s\n",
      "  4600K .......... .......... .......... .......... .......... 49% 6.08M 2s\n",
      "  4650K .......... .......... .......... .......... .......... 50% 1.75M 2s\n",
      "  4700K .......... .......... .......... .......... .......... 50% 3.04M 2s\n",
      "  4750K .......... .......... .......... .......... .......... 51% 3.02M 2s\n",
      "  4800K .......... .......... .......... .......... .......... 52% 2.44M 2s\n",
      "  4850K .......... .......... .......... .......... .......... 52% 2.05M 2s\n",
      "  4900K .......... .......... .......... .......... .......... 53% 3.06M 2s\n",
      "  4950K .......... .......... .......... .......... .......... 53% 3.88M 2s\n",
      "  5000K .......... .......... .......... .......... .......... 54% 2.08M 2s\n",
      "  5050K .......... .......... .......... .......... .......... 54% 2.02M 2s\n",
      "  5100K .......... .......... .......... .......... .......... 55% 4.06M 2s\n",
      "  5150K .......... .......... .......... .......... .......... 55% 2.46M 2s\n",
      "  5200K .......... .......... .......... .......... .......... 56% 3.04M 2s\n",
      "  5250K .......... .......... .......... .......... .......... 56% 2.44M 2s\n",
      "  5300K .......... .......... .......... .......... .......... 57% 4.05M 2s\n",
      "  5350K .......... .......... .......... .......... .......... 57% 2.45M 1s\n",
      "  5400K .......... .......... .......... .......... .......... 58% 3.97M 1s\n",
      "  5450K .......... .......... .......... .......... .......... 59% 6.03M 1s\n",
      "  5500K .......... .......... .......... .......... .......... 59%  160M 1s\n",
      "  5550K .......... .......... .......... .......... .......... 60%  151M 1s\n",
      "  5600K .......... .......... .......... .......... .......... 60%  142M 1s\n",
      "  5650K .......... .......... .......... .......... .......... 61%  162M 1s\n",
      "  5700K .......... .......... .......... .......... .......... 61%  162M 1s\n",
      "  5750K .......... .......... .......... .......... .......... 62%  149M 1s\n",
      "  5800K .......... .......... .......... .......... .......... 62%  162M 1s\n",
      "  5850K .......... .......... .......... .......... .......... 63%  118M 1s\n",
      "  5900K .......... .......... .......... .......... .......... 63%  142M 1s\n",
      "  5950K .......... .......... .......... .......... .......... 64%  161M 1s\n",
      "  6000K .......... .......... .......... .......... .......... 64% 5.41M 1s\n",
      "  6050K .......... .......... .......... .......... .......... 65% 4.47M 1s\n",
      "  6100K .......... .......... .......... .......... .......... 66% 4.85M 1s\n",
      "  6150K .......... .......... .......... .......... .......... 66% 4.52M 1s\n",
      "  6200K .......... .......... .......... .......... .......... 67% 5.05M 1s\n",
      "  6250K .......... .......... .......... .......... .......... 67% 4.05M 1s\n",
      "  6300K .......... .......... .......... .......... .......... 68% 6.09M 1s\n",
      "  6350K .......... .......... .......... .......... .......... 68% 6.04M 1s\n",
      "  6400K .......... .......... .......... .......... .......... 69% 6.09M 1s\n",
      "  6450K .......... .......... .......... .......... .......... 69% 5.88M 1s\n",
      "  6500K .......... .......... .......... .......... .......... 70% 7.60M 1s\n",
      "  6550K .......... .......... .......... .......... .......... 70% 7.91M 1s\n",
      "  6600K .......... .......... .......... .......... .......... 71% 6.77M 1s\n",
      "  6650K .......... .......... .......... .......... .......... 71% 6.48M 1s\n",
      "  6700K .......... .......... .......... .......... .......... 72% 8.32M 1s\n",
      "  6750K .......... .......... .......... .......... .......... 72% 7.49M 1s\n",
      "  6800K .......... .......... .......... .......... .......... 73% 10.6M 1s\n",
      "  6850K .......... .......... .......... .......... .......... 74% 8.93M 1s\n",
      "  6900K .......... .......... .......... .......... .......... 74% 9.37M 1s\n",
      "  6950K .......... .......... .......... .......... .......... 75% 10.5M 1s\n",
      "  7000K .......... .......... .......... .......... .......... 75% 9.90M 1s\n",
      "  7050K .......... .......... .......... .......... .......... 76% 7.64M 1s\n",
      "  7100K .......... .......... .......... .......... .......... 76% 11.4M 1s\n",
      "  7150K .......... .......... .......... .......... .......... 77% 10.8M 1s\n",
      "  7200K .......... .......... .......... .......... .......... 77% 11.3M 1s\n",
      "  7250K .......... .......... .......... .......... .......... 78% 15.7M 1s\n",
      "  7300K .......... .......... .......... .......... .......... 78% 10.6M 1s\n",
      "  7350K .......... .......... .......... .......... .......... 79% 13.8M 1s\n",
      "  7400K .......... .......... .......... .......... .......... 79% 11.8M 1s\n",
      "  7450K .......... .......... .......... .......... .......... 80% 8.81M 1s\n",
      "  7500K .......... .......... .......... .......... .......... 81% 15.7M 1s\n",
      "  7550K .......... .......... .......... .......... .......... 81% 16.3M 1s\n",
      "  7600K .......... .......... .......... .......... .......... 82% 13.3M 0s\n",
      "  7650K .......... .......... .......... .......... .......... 82% 14.0M 0s\n",
      "  7700K .......... .......... .......... .......... .......... 83% 14.2M 0s\n",
      "  7750K .......... .......... .......... .......... .......... 83% 15.0M 0s\n",
      "  7800K .......... .......... .......... .......... .......... 84% 14.3M 0s\n",
      "  7850K .......... .......... .......... .......... .......... 84% 12.2M 0s\n",
      "  7900K .......... .......... .......... .......... .......... 85% 19.4M 0s\n",
      "  7950K .......... .......... .......... .......... .......... 85% 16.9M 0s\n",
      "  8000K .......... .......... .......... .......... .......... 86% 16.8M 0s\n",
      "  8050K .......... .......... .......... .......... .......... 86% 14.7M 0s\n",
      "  8100K .......... .......... .......... .......... .......... 87% 21.0M 0s\n",
      "  8150K .......... .......... .......... .......... .......... 88% 18.2M 0s\n",
      "  8200K .......... .......... .......... .......... .......... 88% 16.2M 0s\n",
      "  8250K .......... .......... .......... .......... .......... 89% 11.9M 0s\n",
      "  8300K .......... .......... .......... .......... .......... 89% 21.3M 0s\n",
      "  8350K .......... .......... .......... .......... .......... 90% 25.9M 0s\n",
      "  8400K .......... .......... .......... .......... .......... 90% 19.3M 0s\n",
      "  8450K .......... .......... .......... .......... .......... 91% 18.2M 0s\n",
      "  8500K .......... .......... .......... .......... .......... 91% 19.6M 0s\n",
      "  8550K .......... .......... .......... .......... .......... 92% 23.3M 0s\n",
      "  8600K .......... .......... .......... .......... .......... 92% 20.5M 0s\n",
      "  8650K .......... .......... .......... .......... .......... 93% 14.6M 0s\n",
      "  8700K .......... .......... .......... .......... .......... 93% 21.2M 0s\n",
      "  8750K .......... .......... .......... .......... .......... 94% 22.6M 0s\n",
      "  8800K .......... .......... .......... .......... .......... 94% 28.6M 0s\n",
      "  8850K .......... .......... .......... .......... .......... 95% 18.2M 0s\n",
      "  8900K .......... .......... .......... .......... .......... 96% 22.9M 0s\n",
      "  8950K .......... .......... .......... .......... .......... 96% 25.3M 0s\n",
      "  9000K .......... .......... .......... .......... .......... 97% 17.2M 0s\n",
      "  9050K .......... .......... .......... .......... .......... 97% 19.1M 0s\n",
      "  9100K .......... .......... .......... .......... .......... 98% 25.3M 0s\n",
      "  9150K .......... .......... .......... .......... .......... 98% 28.0M 0s\n",
      "  9200K .......... .......... .......... .......... .......... 99% 24.7M 0s\n",
      "  9250K .......... .......... .......... .......... .......... 99% 19.9M 0s\n",
      "  9300K .......... .......                                    100% 12.6M=2.4s\n",
      "\n",
      "2025-12-17 08:07:38 (3.84 MB/s) - ‘eng-fra.txt’ saved [9541158/9541158]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset, save to eng-fra.txt\n",
    "import os\n",
    "os.system(\"wget -O eng-fra.txt 'https://cloud.tsinghua.edu.cn/f/d4578477500747d39855/?dl=1'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Helpers\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "# Word Dictionary\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<UNK>\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "        self.min_freq = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = UNK_token\n",
    "            self.word2count[word] = 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        if self.word2count[word] == self.min_freq:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "    def get_index(self, word):\n",
    "        return self.word2index.get(word, UNK_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 95170 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 7874\n",
      "eng 5506\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "def filterPair(p:Tuple[str, str]):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \\\n",
    "        # and p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def readLangs(data_file, lang1_name, lang2_name, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    with open(data_file, mode='r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2_name)\n",
    "        output_lang = Lang(lang1_name)\n",
    "    else:\n",
    "        input_lang = Lang(lang1_name)\n",
    "        output_lang = Lang(lang1_name)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = readLangs('eng-fra.txt', 'eng', 'fra', reverse=True)\n",
    "print(\"Read %s sentence pairs\" % len(pairs))\n",
    "pairs = filterPairs(pairs)\n",
    "print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "print(\"Counting words...\")\n",
    "for pair in pairs:\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "print(\"Counted words:\")\n",
    "print(input_lang.name, input_lang.n_words)\n",
    "print(output_lang.name, output_lang.n_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集搭建\n",
    "完成预处理后，我们得到了一组序列对，所有序列的长度均不超过`MAX_LENGTH`。\n",
    "\n",
    "这些序列的长度是互不一致的，这使得我们在后续进行计算时会遇到困难。为此，我们需要在这些序列后面补0，使得序列长度一致，并记录下正确的序列长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64,10,]\n",
      "[1,10,]\n"
     ]
    }
   ],
   "source": [
    "# Build Dataset\n",
    "class TranslateDataset(Dataset):\n",
    "    def __init__(self, pairs, input_lang, output_lang, max_length=MAX_LENGTH, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._pairs = pairs\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.max_length = max_length\n",
    "        self.total_len = len(self._pairs)\n",
    "\n",
    "    def sentence_to_indexes_with_EOS(self, lang:Lang, sentence:str):\n",
    "        \"\"\"Convert the string sentence into a number sequence.\n",
    "        Words in the sentence are separated by spaces.\n",
    "\n",
    "        Args:\n",
    "            lang (Lang): The dictionary.\n",
    "            sentence (str): The sentence to be coverted.\n",
    "\n",
    "        Returns:\n",
    "            jt.Var: the converted sequence.\n",
    "        \"\"\"\n",
    "        # TODO(1): Split the sentence into multiple words \\\n",
    "        # and then convert them into integer sequences `indexes`\n",
    "\n",
    "        # Your code starts here\n",
    "        words = sentence.split(' ')\n",
    "        indexes = [lang.get_index(word) for word in words]\n",
    "        # Your code ends here\n",
    "\n",
    "        indexes.append(EOS_token)\n",
    "        return jt.array(indexes, dtype=jt.int32)\n",
    "\n",
    "    def pad_sentence(self, sentence:jt.Var):\n",
    "        sentence_len = sentence.shape[0]\n",
    "        var = jt.zeros((self.max_length, ), dtype=jt.int32)\n",
    "        weight = jt.zeros((self.max_length), dtype=jt.float32)\n",
    "        length = jt.empty((1, ), dtype=jt.int32)\n",
    "        var[:sentence_len] = sentence\n",
    "        weight[:sentence_len] = 1\n",
    "        length[0] = sentence_len\n",
    "        return var, weight, length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_str, target_str = self._pairs[index]\n",
    "        input_var = self.sentence_to_indexes_with_EOS(self.input_lang, input_str)\n",
    "        target_var = self.sentence_to_indexes_with_EOS(self.output_lang, target_str)\n",
    "        input, input_weight, input_length = self.pad_sentence(input_var)\n",
    "        target, target_weight, target_length = self.pad_sentence(target_var)\n",
    "        return [\n",
    "            input, target, input_weight, target_weight, input_length, target_length\n",
    "        ], (input_str, target_str)\n",
    "\n",
    "    def process_sentence(self, sentence):\n",
    "        input_var = self.sentence_to_indexes_with_EOS(self.input_lang, normalizeString(sentence))\n",
    "        input, input_weight, input_length = self.pad_sentence(input_var)\n",
    "        return input.unsqueeze(0), input_weight.unsqueeze(0), input_length.unsqueeze(0)\n",
    "\n",
    "random.seed(223514)\n",
    "random.shuffle(pairs)\n",
    "n_train = int(len(pairs) * 0.9)\n",
    "train_pairs = pairs[:n_train]\n",
    "test_pairs = pairs[n_train:]\n",
    "train_dataset = TranslateDataset(train_pairs, input_lang, output_lang, batch_size=batch_size, shuffle=True, endless=True)\n",
    "test_dataset = TranslateDataset(test_pairs, input_lang, output_lang, batch_size=1, shuffle=False)\n",
    "for data in train_dataset:\n",
    "    print(data[0][0].shape)\n",
    "    break\n",
    "\n",
    "for data in test_dataset:\n",
    "    print(data[0][0].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder和Decoder\n",
    "`EncoderRNN`和`AttnDecoderRNN`分别为编码器和解码器的单个单元。\n",
    "\n",
    "注：`AttnDecoderRNN`采用了注意力结构，需要利用编码器的所有隐藏层输出进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,5,20,] [1,5,20,]\n",
      "[5,15,] [1,5,20,]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def execute(self, input, hidden):\n",
    "        \"\"\"EncoderRNN\n",
    "\n",
    "        Args:\n",
    "            input (jt.Var): Input tensor with shape [1, B]\n",
    "            hidden (jt.Var): Hidden tensor with shape [1, B, hidden_size]\n",
    "\n",
    "        Returns:\n",
    "            Tuple[jt.Var, jt.Var]: output tensor, new hidden tensor\n",
    "        \"\"\"\n",
    "        assert input.shape[0] == 1\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def execute(self, input:jt.Var, hidden:jt.Var, encoder_outputs:jt.Var):\n",
    "        \"\"\"AttnDecoderRNN\n",
    "\n",
    "        Args:\n",
    "            input (jt.Var): Input tensor with shape [1, B]\n",
    "            hidden (jt.Var): Hidden tensor with shape [1, B, hidden_size]\n",
    "            encoder_outputs (jt.Var): Encoder outputs with shape [B, max_length, hidden_size]\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)       # [1, B, hidden_size]\n",
    "\n",
    "        attn_input = jt.concat([embedded, hidden], dim=-1).squeeze(0)   # [B, hidden_size]\n",
    "        attn_weights = nn.softmax(self.attn(attn_input), dim=1)                 # [B, max_length]\n",
    "        attn_applied = jt.bmm(attn_weights.unsqueeze(1), encoder_outputs)       # [B, 1, hidden_size]\n",
    "\n",
    "        output = jt.concat(\n",
    "            [embedded.squeeze(0), attn_applied.squeeze(1)],\n",
    "            dim=1)                                                              # [B, hidden_size * 2]\n",
    "        output = self.attn_combine(output).unsqueeze(0)                         # [1, B, hidden_size]\n",
    "\n",
    "        output = nn.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.out(output.squeeze(0))                                    # [B, output_size]\n",
    "        output = nn.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "dummy_input = jt.array(np.random.randint(0, 15, (1, 5)))\n",
    "dummy_hidden = jt.randn(1, 5, 20)\n",
    "dummy_encoder_output = jt.randn(5, 10, 20)\n",
    "dummy_rnn = EncoderRNN(15, 20)\n",
    "dummy_output, dummy_o_hidden = dummy_rnn(dummy_input, dummy_hidden)\n",
    "# [1,5,20,] [1,5,20,]\n",
    "print(dummy_output.shape, dummy_o_hidden.shape)\n",
    "dummy_rnn = AttnDecoderRNN(20, 15)\n",
    "dummy_output, dummy_o_hidden = dummy_rnn(dummy_input, dummy_hidden, dummy_encoder_output)\n",
    "# [5,15,] [1,5,20,]\n",
    "print(dummy_output.shape, dummy_o_hidden.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码过程\n",
    "如图，$h_0$为默认初始隐藏层，$x_1, x_2, \\cdots, x_N$为输入序列，$C$为RNN最终输出。\n",
    "\n",
    "需要注意的是，我们每次输入的是一组序列，这些序列的长度可能不一致。我们根据序列的原始长度找出实际的最终输出。\n",
    "\n",
    "![](imgs/encode.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size)\n",
    "        self.decoder = AttnDecoderRNN(hidden_size, output_size, max_length=max_length)\n",
    "        self.max_length = max_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def encode(self, input:jt.Var, input_weights:jt.Var, input_lengths:jt.Var):\n",
    "        \"\"\"Encode the inputs\n",
    "\n",
    "        Args:\n",
    "            input (jt.Var): Input Sequence with shape [B, max_length], int\n",
    "            input_weights (jt.Var): Input weight (valid to be 1) with shape [B, max_length], float\n",
    "            input_lengths (jt.Var): Input length with shape [B, 1], int\n",
    "        \"\"\"\n",
    "        B = input.shape[0]\n",
    "        # Encode\n",
    "        encoder_outputs = []\n",
    "        encoder_hiddens = []\n",
    "        encoder_hidden = jt.zeros([1, B, self.hidden_size])\n",
    "        for i in range(self.max_length):\n",
    "            slice_input = jt.unsqueeze(input[:, i], 0)         # [1, B]\n",
    "            encoder_output, encoder_hidden = self.encoder(slice_input, encoder_hidden)\n",
    "            encoder_outputs.append(encoder_output)\n",
    "            encoder_hiddens.append(encoder_hidden)\n",
    "        encoder_outputs = jt.concat(encoder_outputs, 0)         # [max_length, B, hidden_size]\n",
    "        encoder_outputs = encoder_outputs.permute((1, 0, 2))    # [B, max_length, hidden_size]\n",
    "        encoder_outputs = encoder_outputs * input_weights.unsqueeze(-1)\n",
    "                                                        \n",
    "        # Find the latest hidden layer of every input sequence\n",
    "        encoder_hiddens = jt.concat(encoder_hiddens, 0)         # [max_length, B, hidden_size]\n",
    "        encoder_ends = (input_lengths.squeeze(1) - 1, jt.arange(B))\n",
    "        encoder_hiddens = encoder_hiddens[encoder_ends]         # [B, hidden_size]\n",
    "        return encoder_outputs, encoder_hiddens\n",
    "\n",
    "    def loss(self, input:jt.Var, target:jt.Var,\n",
    "             input_weights:jt.Var, target_weights:jt.Var,\n",
    "             input_lengths:jt.Var, target_lengths:jt.Var,\n",
    "             decode_func:None):\n",
    "        \"\"\"Loss\n",
    "\n",
    "        Args:\n",
    "            input (jt.Var): Input Sequence with shape [B, max_length], int\n",
    "            target (jt.Var): Target Sequence with shape [B, max_length], int\n",
    "            input_weights (jt.Var): Input weight (valid to be 1) with shape [B, max_length], float\n",
    "            target_weights (jt.Var): Target weight (valid to be 1) with shape [B, max_length], float\n",
    "            input_lengths (jt.Var): Input length with shape [B, 1], int\n",
    "            target_lengths (jt.Var): Target length with shape [B, 1], int\n",
    "        \"\"\"\n",
    "        B = input.shape[0]\n",
    "        e_outputs, e_hiddens = self.encode(input, input_weights, input_lengths)\n",
    "        decoder_outputs = decode_func(self.decoder, e_outputs, e_hiddens, target)\n",
    "        assert isinstance(decoder_outputs, jt.Var)\n",
    "        target = target.reshape(B * self.max_length)\n",
    "        target_weights = target_weights.reshape(B * self.max_length)\n",
    "        decoder_outputs = decoder_outputs.reshape(B * self.max_length, self.output_size)\n",
    "        loss = nn.nll_loss(decoder_outputs, target) * target_weights\n",
    "        loss = jt.reshape(loss, (B, self.max_length)).sum(dim=-1, keepdims=True)\n",
    "        return loss.mean()\n",
    "\n",
    "    def generate(self, input:jt.Var, input_weights:jt.Var, input_lengths:jt.Var, decode_func=None):\n",
    "        e_outputs, e_hiddens = self.encode(input, input_weights, input_lengths)\n",
    "        return decode_func(self.decoder, e_outputs, e_hiddens)\n",
    "\n",
    "    def execute(self, *args, **kwargs):\n",
    "        if self.is_training():\n",
    "            return self.loss(*args, **kwargs)\n",
    "        return self.generate(*args, **kwargs)\n",
    "\n",
    "dummy_model = EncoderDecoder(30, 40, 50)\n",
    "dummy_model.train()\n",
    "dummy_input = jt.randint(0, 30, (5, 10))\n",
    "dummy_iweight = jt.ones_like(dummy_input).float32()\n",
    "dummy_ilength = jt.randint(3, 11, (5, 1))\n",
    "dummy_target = jt.randint(0, 40, (5, 10))\n",
    "dummy_oweight = jt.ones_like(dummy_target).float32()\n",
    "dummy_olength = jt.randint(3, 11, (5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Forcing\n",
    "Teacher Forcing思想：在训练过程中，模型的每一步输入使用真实的目标序列（ground truth），而不是模型自己生成的输出\n",
    "\n",
    "![](imgs/teacher_forcing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling Operators(8/51) used: 6.31s eta: 33.9s 10/51) used: 7.32s eta:   30s 15/51) used: 9.32s eta: 22.4s 16/51) used: 10.3s eta: 22.6s 19/51) used: 12.4s eta: 20.8s 21/51) used: 13.4s eta: 19.1s 24/51) used: 15.4s eta: 17.3s 25/51) used: 16.4s eta: 17.1s 28/51) used: 18.4s eta: 15.1s 33/51) used: 19.4s eta: 10.6s 34/51) used: 21.4s eta: 10.7s 36/51) used: 22.4s eta: 9.35s 40/51) used: 23.4s eta: 6.44s 44/51) used: 24.4s eta: 3.89s 45/51) used: 25.4s eta: 3.39s 48/51) used: 26.5s eta: 1.65s 50/51) used: 27.5s eta: 0.549s 51/51) used: 28.5s eta:    0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.195762634277344\n"
     ]
    }
   ],
   "source": [
    "def decode_teacher_forcing(decoder, e_outputs:jt.Var, e_hiddens:jt.Var, target:jt.Var):\n",
    "    \"\"\"Decoding Process with teacher forcing\n",
    "\n",
    "    Args:\n",
    "        e_outputs (jt.Var): Encoder outputs with shape [B, max_length, hidden_size]\n",
    "        e_hiddens (jt.Var): Encoder hiddens with shape [B, hidden_size]\n",
    "        target (jt.Var): Target results with shape [B, max_length], int\n",
    "\n",
    "    Returns:\n",
    "        decoder_outputs (jt.Var): Decoder outputs with shape [B, max_length, output_size]\n",
    "    \"\"\"\n",
    "    B, L, _ = e_outputs.shape\n",
    "    decoder_hidden = e_hiddens.unsqueeze(0)                 # [1, B, hidden_size]\n",
    "    decoder_input = jt.array([[SOS_token]], dtype=jt.int32)\n",
    "    decoder_input = jt.repeat(decoder_input, (1, B))        # [1, B]\n",
    "    decoder_outputs = []\n",
    "    for i in range(L):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, e_outputs)\n",
    "        decoder_outputs.append(decoder_output)\n",
    "        decoder_input = target[:, i].unsqueeze(0)           # [1, B]\n",
    "    decoder_outputs = jt.stack(decoder_outputs, dim=1)      # [B, max_length, output_size]\n",
    "    return decoder_outputs\n",
    "\n",
    "loss = dummy_model(\n",
    "    dummy_input, dummy_target, dummy_iweight, dummy_oweight, dummy_ilength, dummy_olength,\n",
    "    decode_func = decode_teacher_forcing)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-running\n",
    "Free running直接使用上一个时间步的输出作为当前神经元的输入。\n",
    "\n",
    "![](imgs/free_running.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling Operators(1/5) used: 4.31s eta: 17.2s 2/5) used: 5.32s eta: 7.98s 4/5) used: 6.32s eta: 1.58s 5/5) used: 11.3s eta:    0s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.070526123046875\n"
     ]
    }
   ],
   "source": [
    "def decode_free_run(decoder:nn.Module, e_outputs:jt.Var, e_hiddens:jt.Var, target:jt.Var):\n",
    "    \"\"\"Decoding Process Free run\n",
    "\n",
    "    Args:\n",
    "        e_outputs (jt.Var): Encoder outputs with shape [B, max_length, hidden_size]\n",
    "        e_hiddens (jt.Var): Encoder hiddens with shape [B, hidden_size]\n",
    "        target (jt.Var): Target results with shape [B, max_length], int\n",
    "\n",
    "    Returns:\n",
    "        decoder_outputs (jt.Var): Decoder outputs with shape [B, max_length, output_size]\n",
    "    \"\"\"\n",
    "\n",
    "    B, L, _ = e_outputs.shape\n",
    "    decoder_hidden = e_hiddens.unsqueeze(0)                 # [1, B, hidden_size]\n",
    "    decoder_input = jt.array([[SOS_token]], dtype=jt.int32)\n",
    "    decoder_input = jt.repeat(decoder_input, (1, B))        # [1, B]\n",
    "    decoder_outputs = []\n",
    "    # TODO(2): Free Running for model training\n",
    "\n",
    "    # Your code starts here\n",
    "    for i in range(L):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, e_outputs)\n",
    "        decoder_outputs.append(decoder_output)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.transpose(0, 1)  # [1, B]\n",
    "    # Your code ends here\n",
    "\n",
    "    decoder_outputs = jt.stack(decoder_outputs, dim=1)      # [B, max_length, output_size]\n",
    "    return decoder_outputs\n",
    "loss = dummy_model(\n",
    "    dummy_input, dummy_target, dummy_iweight, dummy_oweight, dummy_ilength, dummy_olength,\n",
    "    decode_func = decode_free_run)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.328: 100%|██████████| 10000/10000 [10:33<00:00, 15.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rand_training_decode(*args):\n",
    "    if random.random() < 0.5:\n",
    "        return decode_teacher_forcing(*args)\n",
    "    else:\n",
    "        return decode_free_run(*args)\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=2.0)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def train_model(model:nn.Module, optimizer:optim.Optimizer, n_iters, plot_gap):\n",
    "    plot_losses = []\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    pbar = tqdm(train_dataset, total=n_iters)\n",
    "    now_iter = 0\n",
    "    for data in pbar:\n",
    "        data, _ = data\n",
    "        now_iter += 1\n",
    "        loss = model(*data, decode_func=rand_training_decode)\n",
    "        optimizer.step(loss)\n",
    "        loss = loss.item()\n",
    "\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if now_iter % plot_gap == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_gap\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "        pbar.set_description(\"Loss: {:.3f}\".format(loss))\n",
    "        if now_iter > n_iters:\n",
    "            break\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    jt.save(model.state_dict(), 'model.jt')\n",
    "\n",
    "model = EncoderDecoder(input_lang.n_words, output_lang.n_words, hidden_size=512)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "skip_train = False\n",
    "if not skip_train:\n",
    "    train_model(model, optimizer, n_iters=10000, plot_gap=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理阶段的解码\n",
    "贪心方法：与free running类似，将预测概率最大的单词作为输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_greedy(decoder, e_outputs:jt.Var, e_hiddens:jt.Var):\n",
    "    \"\"\"Decode Greedy\n",
    "\n",
    "    Args:\n",
    "        e_outputs (jt.Var): Encoder outputs with shape [B, max_length, hidden_size]\n",
    "        e_hiddens (jt.Var): Encoder hiddens with shape [B, hidden_size]\n",
    "\n",
    "    Returns:\n",
    "        List[Int]: Generated Sentence\n",
    "    \"\"\"\n",
    "    B, L, _ = e_outputs.shape\n",
    "    assert B == 1\n",
    "    decoder_hidden = e_hiddens.unsqueeze(0)                 # [1, B, hidden_size]\n",
    "    decoder_input = jt.array([[SOS_token]], dtype=jt.int32)\n",
    "    decoder_input = jt.repeat(decoder_input, (1, B))        # [1, B]\n",
    "    generated_indices = []\n",
    "    # TODO(3): simple greedy decode for inference\n",
    "    # The process is basically the same as free running, \n",
    "    # except that generated_indices is of type List[int] and its length is equal t the length of the generated sequence\n",
    "    # (i.e., it ends with EOS or the length has reached MAX_LENGTH)\n",
    "\n",
    "    # Your code starts here\n",
    "    for i in range(MAX_LENGTH):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, e_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        generated_word_idx = topi.item() if topi.numel() == 1 else topi.squeeze().item()\n",
    "        generated_indices.append(generated_word_idx)\n",
    "        \n",
    "        if generated_word_idx == EOS_token:\n",
    "            break\n",
    "        \n",
    "        decoder_input = topi.transpose(0, 1)  \n",
    "    # Your code ends here\n",
    "\n",
    "    return generated_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you re a kid child . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def post_process_pred(sequence):\n",
    "    results = [output_lang.index2word[r] for r in sequence]\n",
    "    sentence = \" \".join(results)\n",
    "    return sentence\n",
    "\n",
    "def post_process_target(target_var:jt.Var):\n",
    "    assert target_var.ndim == 1 or (target_var.shape[0] == 1 and target_var.ndim == 2)\n",
    "    if target_var.shape[0] == 1 and target_var.ndim == 2:\n",
    "        target_var = target_var.squeeze(0)\n",
    "    sequence = target_var.numpy().tolist()\n",
    "    while len(sequence) > 0 and sequence[-1] == 0:\n",
    "        sequence.pop()\n",
    "    return post_process_pred(sequence)\n",
    "\n",
    "def process_single(model:nn.Module, decode_func, sentence):\n",
    "    data = test_dataset.process_sentence(sentence)\n",
    "    sequence = model(*data, decode_func=decode_func)\n",
    "    return post_process_pred(sequence)\n",
    "\n",
    "model.eval()\n",
    "model.load_state_dict(jt.load('model.jt'))\n",
    "# source_sentence = 'Bonjour le monde'\n",
    "source_sentence = 'Tu es un enfant délicieux'\n",
    "print(process_single(model, decode_func=decode_greedy, sentence=source_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/9517 [00:00<01:30, 104.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['grandis un peu .']\n",
      "= grow up a little . <EOS>\n",
      "< a little bit . <EOS>\n",
      "\n",
      "> ['tom ravala sa fierte .']\n",
      "= tom swallowed his pride . <EOS>\n",
      "< tom unfolded his pride . <EOS>\n",
      "\n",
      "> ['qu est ce qui m est tombe dessus ?']\n",
      "= what fell on me ? <EOS>\n",
      "< what s on board ? <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9517/9517 [01:39<00:00, 95.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 25.376 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def acc_eval(decode_func, n_show):\n",
    "    cnt_acc, cnt_tot = 0, 0\n",
    "    for eval_id, data in tqdm(enumerate(test_dataset), total=len(test_dataset)):\n",
    "        data, str_pairs = data\n",
    "        o_str, t_str = str_pairs\n",
    "        sequence = model(*data[::2], decode_func=decode_func)\n",
    "        pred = post_process_pred(sequence)\n",
    "        target = post_process_target(data[1])\n",
    "        if eval_id < n_show:\n",
    "            print('>', o_str)\n",
    "            print('=', target)\n",
    "            print('<', pred)\n",
    "            print('')\n",
    "        cnt_tot += 1\n",
    "        if pred == target:\n",
    "            cnt_acc += 1\n",
    "    print(\"acc: {:.3f} %\".format(cnt_acc / cnt_tot * 100))\n",
    "\n",
    "acc_eval(decode_func=decode_greedy, n_show=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
